{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ed53c95-e063-4d62-bfa7-f65d99b6071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 6 Training Data:\n",
      "============================================================\n",
      "Week 1: Input = [0.75 0.75 0.75 0.75 0.75] → Output = -1.520678\n",
      "Week 2: Input = [0.3 0.3 0.3 0.3 0.3] → Output = -1.138616\n",
      "Week 3: Input = [0.489055 0.019291 0.44768  0.402488 0.315673] → Output = -1.123070\n",
      "\n",
      "Method 1: Linear Regression\n",
      "------------------------------------------------------------\n",
      "Method 2: Gradient Boosting\n",
      "------------------------------------------------------------\n",
      "Method 3: Random Forest\n",
      "------------------------------------------------------------\n",
      "Method 4: Gaussian Process\n",
      "------------------------------------------------------------\n",
      "Method 5: Bayesian Optimization\n",
      "------------------------------------------------------------\n",
      "Bayesian Optimization found: [0.001 0.001 0.001 0.001 0.001]\n",
      "\n",
      "\n",
      "Predictions for candidate inputs:\n",
      "============================================================\n",
      "Input                                              LR         GB         RF         GP        \n",
      "------------------------------------------------------------\n",
      "[0.2 0.2 0.2 0.2 0.2]                              -1.053714  -1.138619  -1.153366  -1.047443 \n",
      "[0.25 0.25 0.25 0.25 0.25]                         -1.096165  -1.138619  -1.153366  -1.093191 \n",
      "[0.3 0.3 0.3 0.3 0.3]                              -1.138616  -1.138619  -1.153366  -1.138616 \n",
      "[0.35 0.35 0.35 0.35 0.35]                         -1.181067  -1.134689  -1.151812  -1.183643 \n",
      "[0.4 0.4 0.4 0.4 0.4]                              -1.223519  -1.127171  -1.173336  -1.228196 \n",
      "[0.28 0.32 0.3  0.29 0.31]                         -1.141900  -1.134689  -1.151812  -1.141738 \n",
      "[0.001 0.001 0.001 0.001 0.001]                    -0.884757  -1.134523  -1.151345  -0.863641 \n",
      "\n",
      "Ensemble Predictions (average of all models):\n",
      "------------------------------------------------------------\n",
      "[0.2 0.2 0.2 0.2 0.2]                              → -1.098286\n",
      "[0.25 0.25 0.25 0.25 0.25]                         → -1.120335\n",
      "[0.3 0.3 0.3 0.3 0.3]                              → -1.142305\n",
      "[0.35 0.35 0.35 0.35 0.35]                         → -1.162803\n",
      "[0.4 0.4 0.4 0.4 0.4]                              → -1.188055\n",
      "[0.28 0.32 0.3  0.29 0.31]                         → -1.142535\n",
      "[0.001 0.001 0.001 0.001 0.001]                    → -1.008567\n",
      "\n",
      "============================================================\n",
      "RECOMMENDED Week 4 submission for Function 6: [0.001 0.001 0.001 0.001 0.001]\n",
      "Expected output: -1.008567\n",
      "============================================================\n",
      "\n",
      "Additional Analysis:\n",
      "------------------------------------------------------------\n",
      "Observation: All outputs are negative\n",
      "Week 1: [0.75] (uniform high) → -1.52 (worst)\n",
      "Week 2: [0.30] (uniform medium) → -1.14 (better)\n",
      "Week 3: [varied] → -1.12 (best so far)\n",
      "Pattern: Lower and/or more varied values seem to perform better\n",
      "Goal: Get closest to zero (maximize)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Historical data for Function 6 (weeks 1-3)\n",
    "X = np.array([\n",
    "    [0.75, 0.75, 0.75, 0.75, 0.75],           # Week 1 input\n",
    "    [0.3, 0.3, 0.3, 0.3, 0.3],                # Week 2 input\n",
    "    [0.489055, 0.019291, 0.44768, 0.402488, 0.315673]   # Week 3 input\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    -1.5206776627405865,   # Week 1 output\n",
    "    -1.1386161416875031,   # Week 2 output\n",
    "    -1.1230704411246568    # Week 3 output\n",
    "])\n",
    "\n",
    "print(\"Function 6 Training Data:\")\n",
    "print(\"=\"*60)\n",
    "for i in range(len(X)):\n",
    "    print(f\"Week {i+1}: Input = {X[i]} → Output = {y[i]:.6f}\")\n",
    "print()\n",
    "\n",
    "# Method 1: Linear Regression\n",
    "print(\"Method 1: Linear Regression\")\n",
    "print(\"-\"*60)\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X, y)\n",
    "\n",
    "# Method 2: Gradient Boosting\n",
    "print(\"Method 2: Gradient Boosting\")\n",
    "print(\"-\"*60)\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X, y)\n",
    "\n",
    "# Method 3: Random Forest\n",
    "print(\"Method 3: Random Forest\")\n",
    "print(\"-\"*60)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Method 4: Gaussian Process\n",
    "print(\"Method 4: Gaussian Process\")\n",
    "print(\"-\"*60)\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))\n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n",
    "gp_model.fit(X, y)\n",
    "\n",
    "# Method 5: Bayesian Optimization using GP\n",
    "print(\"Method 5: Bayesian Optimization\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "def acquisition_function(x, model):\n",
    "    \"\"\"Expected Improvement acquisition function\"\"\"\n",
    "    x = x.reshape(1, -1)\n",
    "    mu, sigma = model.predict(x, return_std=True)\n",
    "    # We want to maximize, so we negate for minimization\n",
    "    return -mu[0]\n",
    "\n",
    "# Use Bayesian Optimization to find best input\n",
    "best_score = float('-inf')\n",
    "best_input_bo = None\n",
    "\n",
    "# Try multiple random starts for optimization\n",
    "for _ in range(30):\n",
    "    x0 = np.random.uniform(0.001, 0.5, 5)\n",
    "    \n",
    "    result = minimize(\n",
    "        lambda x: acquisition_function(x, gp_model),\n",
    "        x0,\n",
    "        method='L-BFGS-B',\n",
    "        bounds=[(0.001, 1.0)] * 5\n",
    "    )\n",
    "    \n",
    "    if result.success:\n",
    "        pred_value = gp_model.predict(result.x.reshape(1, -1))[0]\n",
    "        if pred_value > best_score:\n",
    "            best_score = pred_value\n",
    "            best_input_bo = result.x\n",
    "\n",
    "print(f\"Bayesian Optimization found: {best_input_bo}\")\n",
    "print()\n",
    "\n",
    "# Test candidate inputs\n",
    "candidates = np.array([\n",
    "    [0.2, 0.2, 0.2, 0.2, 0.2],\n",
    "    [0.25, 0.25, 0.25, 0.25, 0.25],\n",
    "    [0.3, 0.3, 0.3, 0.3, 0.3],\n",
    "    [0.35, 0.35, 0.35, 0.35, 0.35],\n",
    "    [0.4, 0.4, 0.4, 0.4, 0.4],\n",
    "    [0.28, 0.32, 0.3, 0.29, 0.31],  # Slightly varied\n",
    "    best_input_bo  # Add BO result\n",
    "])\n",
    "\n",
    "print(\"\\nPredictions for candidate inputs:\")\n",
    "print(\"=\"*60)\n",
    "print(f\"{'Input':<50} {'LR':<10} {'GB':<10} {'RF':<10} {'GP':<10}\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "all_predictions = []\n",
    "for candidate in candidates:\n",
    "    lr_pred = lr_model.predict([candidate])[0]\n",
    "    gb_pred = gb_model.predict([candidate])[0]\n",
    "    rf_pred = rf_model.predict([candidate])[0]\n",
    "    gp_pred = gp_model.predict([candidate])[0]\n",
    "    \n",
    "    avg_pred = (lr_pred + gb_pred + rf_pred + gp_pred) / 4\n",
    "    all_predictions.append(avg_pred)\n",
    "    \n",
    "    print(f\"{str(candidate):<50} {lr_pred:<10.6f} {gb_pred:<10.6f} {rf_pred:<10.6f} {gp_pred:<10.6f}\")\n",
    "\n",
    "print()\n",
    "print(\"Ensemble Predictions (average of all models):\")\n",
    "print(\"-\"*60)\n",
    "for i, candidate in enumerate(candidates):\n",
    "    print(f\"{str(candidate):<50} → {all_predictions[i]:.6f}\")\n",
    "\n",
    "# Choose best based on ensemble\n",
    "best_idx = np.argmax(all_predictions)\n",
    "best_input = candidates[best_idx]\n",
    "\n",
    "print()\n",
    "print(\"=\"*60)\n",
    "print(f\"RECOMMENDED Week 4 submission for Function 6: {best_input}\")\n",
    "print(f\"Expected output: {all_predictions[best_idx]:.6f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Additional analysis\n",
    "print(\"\\nAdditional Analysis:\")\n",
    "print(\"-\"*60)\n",
    "print(\"Observation: All outputs are negative\")\n",
    "print(\"Week 1: [0.75] (uniform high) → -1.52 (worst)\")\n",
    "print(\"Week 2: [0.30] (uniform medium) → -1.14 (better)\")\n",
    "print(\"Week 3: [varied] → -1.12 (best so far)\")\n",
    "print(\"Pattern: Lower and/or more varied values seem to perform better\")\n",
    "print(\"Goal: Get closest to zero (maximize)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86b4c681-8cfe-463f-b9ba-60f686419b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Recommended Submission for Function 6 (Week 4) ---\n",
      "Predicted Optimal Input: [0.688569 0.001    0.03538  0.001    0.001   ]\n",
      "Predicted Output Value:  -0.861688\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, WhiteKernel\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# 1. DATA PREPARATION (Extracted from your files)\n",
    "# Inputs for Function 6 (3 weeks of history)\n",
    "X = np.array([\n",
    "    [0.75, 0.75, 0.75, 0.75, 0.75],                 # Week 1\n",
    "    [0.3, 0.3, 0.3, 0.3, 0.3],                      # Week 2\n",
    "    [0.489055, 0.019291, 0.44768, 0.402488, 0.315673] # Week 3\n",
    "])\n",
    "\n",
    "# Outputs for Function 6\n",
    "y = np.array([\n",
    "    -1.5206776627405865,\n",
    "    -1.1386161416875031,\n",
    "    -1.1230704411246568\n",
    "])\n",
    "\n",
    "# 2. MODEL DEFINITION\n",
    "# We use a Gaussian Process with a Radial Basis Function (RBF) kernel.\n",
    "# We include a WhiteKernel to account for potential noise in the observations.\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + \\\n",
    "         WhiteKernel(noise_level=1e-5, noise_level_bounds=(1e-10, 1e-1))\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, normalize_y=True)\n",
    "\n",
    "# 3. TRAINING\n",
    "gp.fit(X, y)\n",
    "\n",
    "# 4. OPTIMIZATION (Bayesian Optimization Step)\n",
    "# We want to find x that maximizes the GP prediction. \n",
    "# Since scipy.minimize finds the minimum, we minimize the negative prediction.\n",
    "def objective_function(x):\n",
    "    # Reshape x to (1, -1) as predict expects a 2D array\n",
    "    prediction, sigma = gp.predict(x.reshape(1, -1), return_std=True)\n",
    "    \n",
    "    # Exploration vs Exploitation (UCB - Upper Confidence Bound)\n",
    "    # Kappa controls the balance. Higher kappa = more exploration.\n",
    "    kappa = 1.96 \n",
    "    return -(prediction + kappa * sigma) # Negative because we want to maximize\n",
    "\n",
    "# Constraints: Inputs must be positive. \n",
    "# We assume a reasonable search bound of [0.0, 1.0] based on previous data.\n",
    "# If you suspect values can be higher, change 1.0 to a higher number.\n",
    "bounds = [(0.001, 1.0) for _ in range(5)]\n",
    "\n",
    "# Start the search from the best point we have found so far (Week 3)\n",
    "x0 = X[2]\n",
    "\n",
    "# Run the optimizer\n",
    "res = minimize(\n",
    "    objective_function, \n",
    "    x0, \n",
    "    bounds=bounds, \n",
    "    method='L-BFGS-B'\n",
    ")\n",
    "\n",
    "# 5. RESULT\n",
    "print(\"--- Recommended Submission for Function 6 (Week 4) ---\")\n",
    "print(\"Predicted Optimal Input:\", np.round(res.x, 6))\n",
    "print(\"Predicted Output Value: \", np.round(-res.fun, 6)) # Note: this is the UCB value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77044ba1-db03-448c-a4e3-856de42de604",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
