{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ed06d45-48b3-4e42-8554-1cc45d85625d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function 8 Training Data:\n",
      "============================================================\n",
      "Week 1: Input = [0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1]\n",
      "         Output = 9.54230000\n",
      "Week 2: Input = [0.12 0.09 0.11 0.1  0.08 0.13 0.11 0.09]\n",
      "         Output = 9.55359000\n",
      "Week 3: Input = [0.288486 0.250604 0.018629 0.285988 0.135625 0.216996 0.248538 0.295101]\n",
      "         Output = 9.54832511\n",
      "\n",
      "Method 1: Linear Regression\n",
      "------------------------------------------------------------\n",
      "Method 2: Gradient Boosting\n",
      "------------------------------------------------------------\n",
      "Method 3: Random Forest\n",
      "------------------------------------------------------------\n",
      "Method 4: Neural Network\n",
      "------------------------------------------------------------\n",
      "Method 5: Gaussian Process\n",
      "------------------------------------------------------------\n",
      "Method 6: Bayesian Optimization\n",
      "------------------------------------------------------------\n",
      "Bayesian Optimization found: [1.    0.001 1.    0.001 0.001 1.    1.    0.001]\n",
      "\n",
      "\n",
      "Predictions for candidate inputs:\n",
      "================================================================================\n",
      "Input (first 4 values)         LR           GB           RF           NN           GP          \n",
      "--------------------------------------------------------------------------------\n",
      "[0.100, 0.100, 0.100, 0.100...] 9.542300     9.542300     9.545251     8.974656     9.542300    \n",
      "[0.150, 0.150, 0.150, 0.150...] 9.546585     9.552710     9.549511     9.846258     9.546629    \n",
      "[0.120, 0.120, 0.120, 0.120...] 9.544014     9.552710     9.549511     9.323297     9.544039    \n",
      "[0.080, 0.080, 0.080, 0.080...] 9.540586     9.542300     9.545702     8.626015     9.540550    \n",
      "[0.110, 0.090, 0.100, 0.120...] 9.549840     9.545646     9.547170     9.002964     9.549849    \n",
      "[0.130, 0.110, 0.090, 0.100...] 9.540146     9.550223     9.547689     9.175118     9.540161    \n",
      "[1.000, 0.001, 1.000, 0.001...] 9.888756     9.552069     9.549647     19.449531    9.883617    \n",
      "\n",
      "Ensemble Predictions (average of all models):\n",
      "--------------------------------------------------------------------------------\n",
      "[0.100, 0.100, 0.100, 0.100...] → 9.42936131\n",
      "[0.150, 0.150, 0.150, 0.150...] → 9.60833859\n",
      "[0.120, 0.120, 0.120, 0.120...] → 9.50271417\n",
      "[0.080, 0.080, 0.080, 0.080...] → 9.35903067\n",
      "[0.110, 0.090, 0.100, 0.120...] → 9.43909384\n",
      "[0.130, 0.110, 0.090, 0.100...] → 9.47066729\n",
      "[1.000, 0.001, 1.000, 0.001...] → 11.66472394\n",
      "\n",
      "================================================================================\n",
      "RECOMMENDED Week 4 submission for Function 8:\n",
      "[1.    0.001 1.    0.001 0.001 1.    1.    0.001]\n",
      "Expected output: 11.66472394\n",
      "================================================================================\n",
      "\n",
      "Additional Analysis:\n",
      "--------------------------------------------------------------------------------\n",
      "Observation: Function 8 outputs are all very close (around 9.54-9.55)\n",
      "Week 1: [0.1] (uniform) → 9.5423\n",
      "Week 2: [varied ~0.1] → 9.55359 (BEST so far)\n",
      "Week 3: [more varied] → 9.5483\n",
      "Pattern: Small variations around 0.1-0.15 range, with Week 2's approach best\n",
      "The function seems relatively stable - maximizing requires fine-tuning\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C\n",
    "from scipy.optimize import minimize\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Historical data for Function 8 (weeks 1-3)\n",
    "X = np.array([\n",
    "    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],           # Week 1 input\n",
    "    [0.12, 0.09, 0.11, 0.1, 0.08, 0.13, 0.11, 0.09],    # Week 2 input\n",
    "    [0.288486, 0.250604, 0.018629, 0.285988, 0.135625, 0.216996, 0.248538, 0.295101]   # Week 3 input\n",
    "])\n",
    "\n",
    "y = np.array([\n",
    "    9.5423,           # Week 1 output\n",
    "    9.55359,          # Week 2 output\n",
    "    9.5483251064884   # Week 3 output\n",
    "])\n",
    "\n",
    "print(\"Function 8 Training Data:\")\n",
    "print(\"=\"*60)\n",
    "for i in range(len(X)):\n",
    "    print(f\"Week {i+1}: Input = {X[i]}\")\n",
    "    print(f\"         Output = {y[i]:.8f}\")\n",
    "print()\n",
    "\n",
    "# Method 1: Linear Regression\n",
    "print(\"Method 1: Linear Regression\")\n",
    "print(\"-\"*60)\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X, y)\n",
    "\n",
    "# Method 2: Gradient Boosting\n",
    "print(\"Method 2: Gradient Boosting\")\n",
    "print(\"-\"*60)\n",
    "gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)\n",
    "gb_model.fit(X, y)\n",
    "\n",
    "# Method 3: Random Forest\n",
    "print(\"Method 3: Random Forest\")\n",
    "print(\"-\"*60)\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(X, y)\n",
    "\n",
    "# Method 4: Neural Network\n",
    "print(\"Method 4: Neural Network\")\n",
    "print(\"-\"*60)\n",
    "nn_model = MLPRegressor(hidden_layer_sizes=(4, 4), max_iter=2000, random_state=42)\n",
    "nn_model.fit(X, y)\n",
    "\n",
    "# Method 5: Gaussian Process\n",
    "print(\"Method 5: Gaussian Process\")\n",
    "print(\"-\"*60)\n",
    "kernel = C(1.0, (1e-3, 1e3)) * RBF(1.0, (1e-3, 1e3))\n",
    "gp_model = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n",
    "gp_model.fit(X, y)\n",
    "\n",
    "# Method 6: Bayesian Optimization using GP\n",
    "print(\"Method 6: Bayesian Optimization\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "def acquisition_function(x, model):\n",
    "    \"\"\"Expected Improvement acquisition function\"\"\"\n",
    "    x = x.reshape(1, -1)\n",
    "    mu, sigma = model.predict(x, return_std=True)\n",
    "    # We want to maximize, so we negate for minimization\n",
    "    return -mu[0]\n",
    "\n",
    "# Use Bayesian Optimization to find best input\n",
    "best_score = float('-inf')\n",
    "best_input_bo = None\n",
    "\n",
    "# Try multiple random starts for optimization\n",
    "for _ in range(40):\n",
    "    x0 = np.random.uniform(0.001, 0.5, 8)\n",
    "    \n",
    "    result = minimize(\n",
    "        lambda x: acquisition_function(x, gp_model),\n",
    "        x0,\n",
    "        method='L-BFGS-B',\n",
    "        bounds=[(0.001, 1.0)] * 8\n",
    "    )\n",
    "    \n",
    "    if result.success:\n",
    "        pred_value = gp_model.predict(result.x.reshape(1, -1))[0]\n",
    "        if pred_value > best_score:\n",
    "            best_score = pred_value\n",
    "            best_input_bo = result.x\n",
    "\n",
    "print(f\"Bayesian Optimization found: {best_input_bo}\")\n",
    "print()\n",
    "\n",
    "# Test candidate inputs\n",
    "candidates = np.array([\n",
    "    [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1],\n",
    "    [0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15, 0.15],\n",
    "    [0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12],\n",
    "    [0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08],\n",
    "    [0.11, 0.09, 0.1, 0.12, 0.08, 0.13, 0.1, 0.11],  # Varied around 0.1\n",
    "    [0.13, 0.11, 0.09, 0.1, 0.12, 0.08, 0.14, 0.1],  # Different variation\n",
    "    best_input_bo  # Add BO result\n",
    "])\n",
    "\n",
    "print(\"\\nPredictions for candidate inputs:\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Input (first 4 values)':<30} {'LR':<12} {'GB':<12} {'RF':<12} {'NN':<12} {'GP':<12}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "all_predictions = []\n",
    "for candidate in candidates:\n",
    "    lr_pred = lr_model.predict([candidate])[0]\n",
    "    gb_pred = gb_model.predict([candidate])[0]\n",
    "    rf_pred = rf_model.predict([candidate])[0]\n",
    "    nn_pred = nn_model.predict([candidate])[0]\n",
    "    gp_pred = gp_model.predict([candidate])[0]\n",
    "    \n",
    "    avg_pred = (lr_pred + gb_pred + rf_pred + nn_pred + gp_pred) / 5\n",
    "    all_predictions.append(avg_pred)\n",
    "    \n",
    "    cand_str = f\"[{candidate[0]:.3f}, {candidate[1]:.3f}, {candidate[2]:.3f}, {candidate[3]:.3f}...]\"\n",
    "    print(f\"{cand_str:<30} {lr_pred:<12.6f} {gb_pred:<12.6f} {rf_pred:<12.6f} {nn_pred:<12.6f} {gp_pred:<12.6f}\")\n",
    "\n",
    "print()\n",
    "print(\"Ensemble Predictions (average of all models):\")\n",
    "print(\"-\"*80)\n",
    "for i, candidate in enumerate(candidates):\n",
    "    cand_str = f\"[{candidate[0]:.3f}, {candidate[1]:.3f}, {candidate[2]:.3f}, {candidate[3]:.3f}...]\"\n",
    "    print(f\"{cand_str:<30} → {all_predictions[i]:.8f}\")\n",
    "\n",
    "# Choose best based on ensemble\n",
    "best_idx = np.argmax(all_predictions)\n",
    "best_input = candidates[best_idx]\n",
    "\n",
    "print()\n",
    "print(\"=\"*80)\n",
    "print(f\"RECOMMENDED Week 4 submission for Function 8:\")\n",
    "print(f\"{best_input}\")\n",
    "print(f\"Expected output: {all_predictions[best_idx]:.8f}\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Additional analysis\n",
    "print(\"\\nAdditional Analysis:\")\n",
    "print(\"-\"*80)\n",
    "print(\"Observation: Function 8 outputs are all very close (around 9.54-9.55)\")\n",
    "print(\"Week 1: [0.1] (uniform) → 9.5423\")\n",
    "print(\"Week 2: [varied ~0.1] → 9.55359 (BEST so far)\")\n",
    "print(\"Week 3: [more varied] → 9.5483\")\n",
    "print(\"Pattern: Small variations around 0.1-0.15 range, with Week 2's approach best\")\n",
    "print(\"The function seems relatively stable - maximizing requires fine-tuning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8786b8-97af-4043-8325-6d796e3c448a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
